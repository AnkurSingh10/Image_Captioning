{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fb57c9-f7eb-455c-8bfb-a8c63c68ac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import random\n",
    "import string\n",
    "from pickle import dump, load\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import (Input, Dense, Reshape, Embedding, Concatenate,\n",
    "                                     Dropout, LayerNormalization, GlobalAveragePooling1D , Layer, MultiHeadAttention)\n",
    "from tensorflow.keras.utils import to_categorical, Sequence, pad_sequences,get_file\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Lambda\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.applications.resnet import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import plot_model\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from Custom_layer_model import Transformer_decoder, PositionalEmbedding, Masked_Loss, Expand_Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c20135-15bd-42af-ba9d-84dfc99bdc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"flickr30k/flickr30k_images\"\n",
    "caption = pd.read_csv(\"flickr30k/captions.txt\")\n",
    "vit_url = \"https://tfhub.dev/sayakpaul/vit_b16_fe/1\"\n",
    "vit_model = hub.KerasLayer(vit_url, trainable=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19f8cfe-cb95-46db-b1c8-0719732720eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_caption(caption):\n",
    "    cap = caption.lower().strip()\n",
    "    if cap.startswith(\"startseq\") and cap.endswith(\"endseq\"):\n",
    "        mid = cap[len(\"startseq\"):-len(\"endseq\")].strip()\n",
    "    else:\n",
    "        mid = cap\n",
    "    mid = re.sub(r'[^a-z\\s]', '', mid)\n",
    "    mid = re.sub(r'\\s+', ' ', mid).strip()\n",
    "    return f\"startseq {mid} endseq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec368158-bdd9-4115-82a5-c31c9dbb4cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption['comment'] = caption['comment'].apply(clean_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f512d7-c7e6-4d78-9455-d0a70dcff584",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feca8244-c2fc-4d20-a210-2747d7e32f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_list = caption['comment'].tolist()\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(captions_list)\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab29a96b-5b11-4728-8916-76ef6f50f997",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load(open(\"tokenizer.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fb6ad5-4591-4234-b23d-3dff2bc04766",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = min(40,max(len(c.split()) for c in captions_list))\n",
    "\n",
    "print(f\"Vocab size: {vocab_size} \\n Max Caption length: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3680fcf-e100-4cd1-b286-9fc7eef37181",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = caption['image_name'].unique().tolist()\n",
    "train_ids, test_ids = train_test_split(image_ids, test_size=0.1, random_state=42)\n",
    "train_ids, val_ids = train_test_split(train_ids, test_size=0.15, random_state=42)\n",
    "\n",
    "train_df = caption[caption['image_name'].isin(train_ids)].reset_index(drop=True)\n",
    "val_df   = caption[caption['image_name'].isin(val_ids)].reset_index(drop=True)\n",
    "test_df  = caption[caption['image_name'].isin(test_ids)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50be8df3-5eb2-4934-869b-efd9033c8336",
   "metadata": {},
   "source": [
    "##  Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108989dd-d6d7-4cdd-9329-e45e9291b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df, image_dir):\n",
    "    features = {}\n",
    "    for img_name in tqdm(df['image_name'].unique()):\n",
    "        img_path = os.path.join(image_dir, img_name)\n",
    "        img = keras.preprocessing.image.load_img(img_path, target_size=(224,224))\n",
    "        img = keras.preprocessing.image.img_to_array(img) / 255.0\n",
    "        img = np.expand_dims(img, 0)\n",
    "        feat = vit_model(img)  # (1, 768)\n",
    "        features[img_name] = feat.numpy()[0]  # (768,)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433c88c7-49ac-4a66-a88f-55772dd79126",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = extract_features(\n",
    "     caption,\n",
    "     image_dir = image_path\n",
    ")\n",
    "\n",
    "dump(features, open(\"features.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c597349f-ff2c-44b3-89e0-54688a7ffb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = load(open(\"features.pkl\",\"rb\"))\n",
    "for k in features:\n",
    "    features[k] = np.squeeze(features[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8844095-a2f2-453c-9bb2-503ad9614575",
   "metadata": {},
   "source": [
    "## DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d932d197-935e-4e05-92e4-1637fb16f7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, df, features, tokenizer, max_length, batch_size=32, shuffle=True,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.df = df.copy().reset_index(drop=True)\n",
    "        self.features = features\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(self.df))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / self.batch_size))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch = self.df.iloc[batch_indices]\n",
    "        \n",
    "        x_img, x_seq, y = [], [], []\n",
    "        for _, row in batch.iterrows():\n",
    "            img_feature = self.features[row['image_name']]  \n",
    "            seq = self.tokenizer.texts_to_sequences([row['comment']])[0]\n",
    "            seq = seq[:self.max_length]  \n",
    "            seq_pad = pad_sequences([seq], maxlen=self.max_length, padding='post')[0]\n",
    "            \n",
    "            x_img.append(img_feature)\n",
    "            # caption input\n",
    "            x_seq.append(seq_pad[:-1])\n",
    "            # target output\n",
    "            y.append(seq_pad[1:])      \n",
    "        \n",
    "        x_img = np.array(x_img, dtype=np.float32)\n",
    "        x_seq = np.array(x_seq, dtype=np.int32)\n",
    "        y     = np.array(y,     dtype=np.int32)\n",
    "        return (x_img, x_seq), y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296efc32-b24b-49d3-b125-ae8f55b13e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = CustomDataGenerator(train_df, features, tokenizer, max_length, batch_size=32, shuffle=True)\n",
    "val_generator   = CustomDataGenerator(val_df, features, tokenizer, max_length, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e6ac38-b92e-4d05-a700-efb2769cbba6",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993ca5f2-21b8-436f-8249-cf5ade501f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(\n",
    "    vocab_size,\n",
    "    max_length,\n",
    "    emb_dimension=256,\n",
    "    ff_dimension=512,\n",
    "    num_heads=8,\n",
    "    num_layers=4,\n",
    "    \n",
    "):\n",
    "    img_input = Input(shape=(768,), name=\"image_features\")\n",
    "    cap_input = Input(shape=(max_length-1,), name=\"caption_input\")\n",
    "    \n",
    "    img_emb = Expand_Dimension(name=\"image_context\")(img_input)\n",
    "    decoder = Transformer_decoder(embed_dim=embed_dimension, ff_dim=ff_dimension, num_heads=num_heads,\n",
    "                                 vocab_size=vocab_size, max_len=max_length-1,\n",
    "                                 num_layers=num_layers, rate=0.1)\n",
    "    \n",
    "    outputs = decoder(cap_input, img_emb) \n",
    "    \n",
    "    model = Model(inputs=[img_input, cap_input], outputs=outputs)\n",
    "    model.summary()\n",
    "    return model\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7abe49e-6fc9-4ced-aa61-a636a3b89e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model(\n",
    "    vocab_size=vocab_size,\n",
    "    max_length=max_length,\n",
    ")\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=3e-4), loss=Masked_Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b35dfa-65a1-4c10-8e13-8b0f186a3d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(\n",
    "    model,\n",
    "    to_file=\"caption_model.png\",\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    expand_nested=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95346df6-9379-4e3c-8641-abc52f3f0f3d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daeab39-2c21-4ac1-b07c-858ba9b5f729",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_point = ModelCheckpoint(\"caption_model.keras\", monitor=\"val_loss\",\n",
    "                             save_best_only=True, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5,\n",
    "                          restore_best_weights=True, verbose=1)\n",
    "reducelr  = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5,\n",
    "                              patience=3, verbose=1)\n",
    "callbacks = [\n",
    "    check_point,\n",
    "    reducelr,\n",
    "    early_stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0cee1d-e63d-4cdd-824e-f6d2cec3ef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d210f28-24ac-4dcd-8727-99f8c1842ae8",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba078c9-d244-408f-8d41-e5f18f1658ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"caption_model\",\n",
    "                   custom_objects={\"Transformer_decoder\": Transformer_decoder,\n",
    "                                   \"Positional_Embedding\": Positional_Embedding,\n",
    "                                   \"Masked_Loss\": Masked_Loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b95fd0-607e-46e9-b436-bd88e6c438ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_path, model, tokenizer, max_length):\n",
    "    img = keras.preprocessing.image.load_img(image_path, target_size=(224,224))\n",
    "    img = keras.preprocessing.image.img_to_array(img) / 255.0\n",
    "    img = np.expand_dims(img, 0)\n",
    "    feature = vit_model(img)[0].numpy()    \n",
    "    caption = \"startseq\"\n",
    "    for _ in range(max_length):\n",
    "        seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "        seq = pad_sequences([seq], maxlen=max_length, padding='post')\n",
    "        y_pred = model.predict([feature.reshape(1, -1), seq], verbose=0)\n",
    "        next_index = np.argmax(y_pred[0, len(caption.split())-1])\n",
    "        next_word = tokenizer.index_word.get(next_index, '')\n",
    "        if next_word == '' or next_word == 'endseq':\n",
    "            break\n",
    "        caption += ' ' + next_word\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fff4fa3-f331-4f38-818e-93db3a630351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_prediction(\n",
    "    image_path,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    max_length,\n",
    "    dataframe,\n",
    "    save_image = False\n",
    "):\n",
    "  \n",
    "    image_id =os.path.basename(image_path)\n",
    "    # print(image_id)\n",
    "\n",
    "    # Generate caption \n",
    "    pred_caption = generate_caption(\n",
    "        image_path,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        max_length\n",
    "    )\n",
    "\n",
    "    pred_caption = (\n",
    "        pred_caption\n",
    "        .replace(\"startseq\", \"\")\n",
    "        .replace(\"endseq\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    true_caption = dataframe[dataframe[\"image_name\"] == image_id][\"comment\"].iloc[1]\n",
    "    true_caption = (\n",
    "        true_caption\n",
    "        .replace(\"startseq\", \"\")\n",
    "        .replace(\"endseq\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    title_text = (\n",
    "         \"\\n\\nActual: \\n\"\n",
    "        +\"\\n\".join(textwrap.wrap(true_caption, 60))\n",
    "        +\"\\n\"\n",
    "        +\"Predicted:\\n\"\n",
    "        + \"\\n\".join(textwrap.wrap(pred_caption, 60))\n",
    "    )\n",
    "\n",
    "    plt.title(title_text, fontsize=11)\n",
    "    if save_image:\n",
    "        save_path = os.path.join(f\"{image_id}_caption.png\")\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\", dpi=200)\n",
    "        print(f\"Image saved at: {save_path}\")    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934f9dc0-c27b-43aa-9070-b449e6827c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"flickr30k_images/1007205537.jpg\"\n",
    "show_prediction(\n",
    "    image_url,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    max_length - 1,\n",
    "    test,\n",
    "    save_image = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb82115-8469-4835-82c5-ca1b3033d674",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_tokens(tokens):\n",
    "    return [\n",
    "        t for t in tokens\n",
    "        if t not in {\"startseq\", \"endseq\", \"<pad>\"}\n",
    "    ]\n",
    "\n",
    "test_images = test['image_name'].unique().tolist()\n",
    "random.seed(42)\n",
    "sample_images = random.sample(test_images, min(500, len(test_images)))\n",
    "\n",
    "refs, hyps = [], []\n",
    "smooth = SmoothingFunction().method1\n",
    "\n",
    "for img_id in tqdm(sample_images):\n",
    "\n",
    "    img_loc = \"flickr30k_images\" + \"/\"+img_id\n",
    "    pred_caption = generate_caption(\n",
    "        img_loc,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        max_length -1       \n",
    "    )\n",
    "\n",
    "    hyp_tokens = clean_tokens(pred_caption.split())\n",
    "    hyps.append(hyp_tokens)\n",
    "\n",
    "    true_caps = test_df[test['image_name'] == img_id]['comment'].tolist()\n",
    "    ref_tokens = [clean_tokens(cap.split()) for cap in true_caps]\n",
    "\n",
    "    refs.append(ref_tokens)\n",
    "\n",
    "print(\"BLEU-1:\", corpus_bleu(refs, hyps, weights=(1,0,0,0), smoothing_function=smooth))\n",
    "print(\"BLEU-2:\", corpus_bleu(refs, hyps, weights=(0.5,0.5,0,0), smoothing_function=smooth))\n",
    "print(\"BLEU-3:\", corpus_bleu(refs, hyps, weights=(0.33,0.33,0.33,0), smoothing_function=smooth))\n",
    "print(\"BLEU-4:\", corpus_bleu(refs, hyps, weights=(0.25,0.25,0.25,0.25), smoothing_function=smooth))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
